{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b054537d",
   "metadata": {},
   "source": [
    "# StyleMeUp - Fraud Detection in Online Retail "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab1305c",
   "metadata": {},
   "source": [
    "### Data Science Model training, Pipeline, Deploy and serving"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34258750-3cb8-4226-a028-cc868fc1a3d5",
   "metadata": {},
   "source": [
    "### In this notebook we will use the data enriched and prepared by our data engineers using the IPINFO dataset. \n",
    "##### We will follow following steps -\n",
    "\n",
    "1. use snowpark python to connect with snowflake\n",
    "2. get training dataset\n",
    "3. feature visualization\n",
    "4. Check feature importance\n",
    "5. split training dataset into train and test\n",
    "6. setup transformations\n",
    "7. setup classifier\n",
    "8. build ML Pipeline\n",
    "9. train and test the model\n",
    "10. check model accuracy\n",
    "11. deploy model as Python UDF in snowflake\n",
    "\n",
    "#### Finally, Use model deployed in Snowflake to score and predict data saved in snowflake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e08760",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib_venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950bc0b-0c22-4eb5-988c-a6c34f675d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.types import IntegerType, FloatType, StringType, BooleanType\n",
    "\n",
    "from matplotlib_venn import venn2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utilities.creds import Credentials\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.snowpark import version\n",
    "print(f\"Snowflake snowpark version is : {version.VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca2c1a-b93d-4019-b925-a664051a9d7c",
   "metadata": {},
   "source": [
    "#### Create a session to connect with snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daddfac-ddb2-4140-b371-81a3f25ba26d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = Session.builder.configs(Credentials().__dict__).create()\n",
    "print(session.sql('USE WAREHOUSE LEARNINGSNOWPARKVW').collect())\n",
    "print(session.sql('USE DATABASE LEARNINGSNOWPARKDB').collect())\n",
    "print(session.sql('USE SCHEMA FRAUDDEMO').collect())\n",
    "print(session.sql('SELECT CURRENT_WAREHOUSE(), CURRENT_DATABASE(), CURRENT_SCHEMA()').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54014c9-8ab2-490b-94e9-4323c9f2d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = session.table('enriched_data').sample(n = 20000)\n",
    "df = train_dataset.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e2c12-9de5-4f20-9bd7-5f902add7712",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "### Masked IP feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a6857-99e3-49e3-b4b4-fc6734b460a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "venn2(subsets = (len(df.loc[df['ISFRAUD'] == 1]), \n",
    "                 len(df.loc[df['IS_MASKED'] == 1]), \n",
    "                 len(df.loc[(df['ISFRAUD'] == 1) & (df['IS_MASKED'] == 1)])),\n",
    "      set_labels = ('Fraud', 'Masked IP', 'Fraud & Masked IP'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2c4e6b-97c6-4a7a-ab24-5a001e4ea37d",
   "metadata": {},
   "source": [
    "## Training fraud detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908493d-6999-4e68-bc60-734c6e0e71a3",
   "metadata": {},
   "source": [
    "### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CITY', 'SHIPPING_ZIPCODE', 'SHIPPING_STATE', 'PAYMENT_NETWORK', 'PAYMENT_TYPE', \n",
    "            'IS_MASKED', 'AVG_PRICE_PER_ITEM', 'TOTAL_TRNX_AMOUNT', 'IP_TO_SHIPPING_DISTANCE']\n",
    "encoded_features = ['CITY', 'SHIPPING_ZIPCODE', 'SHIPPING_STATE', 'PAYMENT_NETWORK', 'PAYMENT_TYPE','IS_MASKED']\n",
    "\n",
    "num_feature_fill_na = ['IP_TO_SHIPPING_DISTANCE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1400d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data into dataframe\n",
    "data = session.table('enriched_data').sample(n = 10000)\n",
    "df = pd.DataFrame(data.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30872caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pipeline\n",
    "\n",
    "#transformations\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Model Accuracy\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# split train and test\n",
    "X = df[features]\n",
    "y = df['ISFRAUD'] == True\n",
    "weights = (y==0).sum()/(1.0 *  (y==1).sum())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "\n",
    "# Model Pipeline\n",
    "ord_pipe = make_pipeline(\n",
    "    FunctionTransformer(lambda x: x.astype(str)),\n",
    "    OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    )\n",
    "\n",
    "num_pipe = make_pipeline(\n",
    "    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0),\n",
    "    MinMaxScaler()\n",
    "    )\n",
    "\n",
    "clf = make_pipeline(RandomForestClassifier(random_state=0, n_jobs=-1))\n",
    "\n",
    "model = make_pipeline(ord_pipe, num_pipe, clf)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a6d36",
   "metadata": {},
   "source": [
    "#### Check Model balacne accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Accuracy of our model on test dataset\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "predictions = [round(value) for value in y_pred]\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "print(\"Model testing completed.\\n   - Model Balanced Accuracy: %.2f%%\" % (balanced_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d5a876",
   "metadata": {},
   "source": [
    "#### Check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec640fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, predictions).ravel()\n",
    "\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "\n",
    "accuracy =  (TP+TN) /(TP+FP+TN+FN)\n",
    "\n",
    "print('Accuracy of the classification = {:0.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "perm_importance = permutation_importance(model, X_test, y_test)\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(np.array(features)[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c734ce",
   "metadata": {},
   "source": [
    "# Register model as UDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f1e70d6",
   "metadata": {},
   "source": [
    "## !!!The terms need to be accepted by the ORGADMIN first!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "features = list(X_train.columns)\n",
    "\n",
    "session.add_packages(\"scikit-learn==1.0.2\", \"pandas\", \"numpy\")\n",
    "\n",
    "@udf(name='predict_retail_fraud',is_permanent = True, stage_location = '@UDFSTAGE', replace=True)\n",
    "def predict_retail_fraud(args: list) -> float:\n",
    "    row = pd.DataFrame([args], columns=features)\n",
    "    return model.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65514370",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = session.table(name = 'new_transaction_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import snowflake.snowpark.functions as F\n",
    "new_df.select(new_df.trnx_id, \\\n",
    "              F.call_udf(\"predict_retail_fraud\", F.array_construct(*features)).alias('fraud_flag')) \\\n",
    "        .write.mode('overwrite').saveAsTable('fraud_detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb370b",
   "metadata": {},
   "source": [
    "# Predict fraud in new transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table('fraud_detection').sample(n=10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a394aa3",
   "metadata": {},
   "source": [
    "#### This demo showcase how Data Engineering and Data Science teams at StyleMeUp can use familiar programming concepts and APIs, and a rich ecosystem of open source packages provided by Snowpark for Python to collaborate and build this solution.\n",
    "\n",
    "#### Snowflake marketplace and data exchange offerings quickly let you test and build your models using 1st, 2nd and 3rd party data sets for better accuracy and testing. Without worrying about the logisticts of ingesting, transforming and loading data in your own database\n",
    "\n",
    "#### Some great features in this demo are -\n",
    "\n",
    "1. Using Snowflake native GEOGRAPHY datatypes and ST_DISTANCE geography functions to calculate ip_to_shipping distance. (No need for GeoPandas)\n",
    "2. Load data using pandas data frame (new functionality in Snowpark Python API)\n",
    "3. Create and deploy UDF in snowflake without pickle\n",
    "4. Using snowflake marketplace to quickly use 3rd party datasets without any dataengineering\n",
    "5. Using scikit learn, pandas, NumPy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73c97188",
   "metadata": {},
   "source": [
    "# Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea796b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175c236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "24c4f4bd1ad4c4a65933c1dd4f0fe06a6b9fca4b9695c236df39490c76810698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
